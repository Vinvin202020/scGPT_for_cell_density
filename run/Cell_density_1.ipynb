{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a4fbd7",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model for Cell-type Annotation\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model on a new dataset for the cell type annotation task. We use the Multiple Sclerosis dataset as an example and fine-tune on the pre-trained whole-body model. Please download the dataset folder from https://drive.google.com/drive/folders/1Qd42YNabzyr2pWt9xoY4cVMTAxsNBt4v?usp=sharing\n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on cell-type annotation tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9406b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "c:\\Users\\Alexander\\Desktop\\ML\\scGPT\\scgpt_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Alexander\\Desktop\\ML\\scGPT\\tutorials\\..\\scgpt\\model\\model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "c:\\Users\\Alexander\\Desktop\\ML\\scGPT\\tutorials\\..\\scgpt\\model\\multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a7da37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>x_centroid</th>\n",
       "      <th>y_centroid</th>\n",
       "      <th>transcript_counts</th>\n",
       "      <th>control_probe_counts</th>\n",
       "      <th>control_codeword_counts</th>\n",
       "      <th>unassigned_codeword_counts</th>\n",
       "      <th>deprecated_codeword_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>cell_area</th>\n",
       "      <th>nucleus_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaadpbp-1</td>\n",
       "      <td>206.089813</td>\n",
       "      <td>1495.898193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.456877</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>201.765823</td>\n",
       "      <td>1816.210815</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>49.130002</td>\n",
       "      <td>21.268595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaabbaka-1</td>\n",
       "      <td>179.024506</td>\n",
       "      <td>2167.253906</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>119.618911</td>\n",
       "      <td>74.778753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaabbjoo-1</td>\n",
       "      <td>186.060654</td>\n",
       "      <td>2163.309082</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>94.241097</td>\n",
       "      <td>59.109533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaablchg-1</td>\n",
       "      <td>200.246887</td>\n",
       "      <td>2198.593506</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>120.341411</td>\n",
       "      <td>52.426408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162249</th>\n",
       "      <td>ojaaphhh-1</td>\n",
       "      <td>4552.125977</td>\n",
       "      <td>1643.896484</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.913126</td>\n",
       "      <td>11.153594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162250</th>\n",
       "      <td>ojabeldf-1</td>\n",
       "      <td>4437.434082</td>\n",
       "      <td>1629.141846</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.418750</td>\n",
       "      <td>5.418750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162251</th>\n",
       "      <td>ojacfbid-1</td>\n",
       "      <td>4463.312988</td>\n",
       "      <td>1576.604004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.502500</td>\n",
       "      <td>6.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162252</th>\n",
       "      <td>ojacfhhg-1</td>\n",
       "      <td>4619.915527</td>\n",
       "      <td>1454.322388</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.089532</td>\n",
       "      <td>7.089532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162253</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4620.281250</td>\n",
       "      <td>1528.107666</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.683125</td>\n",
       "      <td>6.683125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162254 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cell_id   x_centroid   y_centroid  transcript_counts  \\\n",
       "0       aaaadpbp-1   206.089813  1495.898193                  0   \n",
       "1       aaaaficg-1   201.765823  1816.210815                 19   \n",
       "2       aaabbaka-1   179.024506  2167.253906                 53   \n",
       "3       aaabbjoo-1   186.060654  2163.309082                 29   \n",
       "4       aaablchg-1   200.246887  2198.593506                 42   \n",
       "...            ...          ...          ...                ...   \n",
       "162249  ojaaphhh-1  4552.125977  1643.896484                  3   \n",
       "162250  ojabeldf-1  4437.434082  1629.141846                  6   \n",
       "162251  ojacfbid-1  4463.312988  1576.604004                  0   \n",
       "162252  ojacfhhg-1  4619.915527  1454.322388                  2   \n",
       "162253  ojacpeii-1  4620.281250  1528.107666                  1   \n",
       "\n",
       "        control_probe_counts  control_codeword_counts  \\\n",
       "0                          0                        0   \n",
       "1                          0                        0   \n",
       "2                          0                        0   \n",
       "3                          0                        0   \n",
       "4                          0                        0   \n",
       "...                      ...                      ...   \n",
       "162249                     0                        0   \n",
       "162250                     0                        0   \n",
       "162251                     0                        0   \n",
       "162252                     0                        0   \n",
       "162253                     0                        0   \n",
       "\n",
       "        unassigned_codeword_counts  deprecated_codeword_counts  total_counts  \\\n",
       "0                                0                           0             0   \n",
       "1                                0                           0            19   \n",
       "2                                0                           0            53   \n",
       "3                                0                           0            29   \n",
       "4                                1                           0            43   \n",
       "...                            ...                         ...           ...   \n",
       "162249                           0                           0             3   \n",
       "162250                           0                           0             6   \n",
       "162251                           0                           0             0   \n",
       "162252                           0                           0             2   \n",
       "162253                           0                           0             1   \n",
       "\n",
       "         cell_area  nucleus_area  \n",
       "0        68.456877           NaN  \n",
       "1        49.130002     21.268595  \n",
       "2       119.618911     74.778753  \n",
       "3        94.241097     59.109533  \n",
       "4       120.341411     52.426408  \n",
       "...            ...           ...  \n",
       "162249   26.913126     11.153594  \n",
       "162250    5.418750      5.418750  \n",
       "162251    6.502500      6.502500  \n",
       "162252    7.089532      7.089532  \n",
       "162253    6.683125      6.683125  \n",
       "\n",
       "[162254 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cells = pd.read_csv('data/cells.csv.gz')\n",
    "cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb63a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 10,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 14,\n",
       " 19,\n",
       " 5,\n",
       " 13,\n",
       " 16,\n",
       " 13,\n",
       " 9,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 5,\n",
       " 15,\n",
       " 6,\n",
       " 7,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 11,\n",
       " 8,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 11,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 17,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 19,\n",
       " 14,\n",
       " 9,\n",
       " 10,\n",
       " 19,\n",
       " 12,\n",
       " 20,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 4,\n",
       " 16,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 10,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 14,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 13,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 13,\n",
       " 11,\n",
       " 6,\n",
       " 9,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 14,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 14,\n",
       " 19,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 24,\n",
       " 19,\n",
       " 21,\n",
       " 19,\n",
       " 11,\n",
       " 17,\n",
       " 13,\n",
       " 9,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 19,\n",
       " 14,\n",
       " 17,\n",
       " 19,\n",
       " 21,\n",
       " 21,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 19,\n",
       " 16,\n",
       " 19,\n",
       " 15,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 16,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 32,\n",
       " 31,\n",
       " 29,\n",
       " 35,\n",
       " 27,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 14,\n",
       " 11,\n",
       " 18,\n",
       " 15,\n",
       " 23,\n",
       " 18,\n",
       " 17,\n",
       " 20,\n",
       " 23,\n",
       " 26,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 29,\n",
       " 29,\n",
       " 12,\n",
       " 24,\n",
       " 23,\n",
       " 29,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 11,\n",
       " 10,\n",
       " 17,\n",
       " 10,\n",
       " 9,\n",
       " 24,\n",
       " 16,\n",
       " 7,\n",
       " 11,\n",
       " 18,\n",
       " 11,\n",
       " 13,\n",
       " 25,\n",
       " 23,\n",
       " 20,\n",
       " 27,\n",
       " 22,\n",
       " 26,\n",
       " 22,\n",
       " 19,\n",
       " 18,\n",
       " 21,\n",
       " 21,\n",
       " 23,\n",
       " 15,\n",
       " 20,\n",
       " 13,\n",
       " 14,\n",
       " 17,\n",
       " 19,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 13,\n",
       " 13,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 19,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 8,\n",
       " 16,\n",
       " 18,\n",
       " 16,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 11,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 17,\n",
       " 7,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 7,\n",
       " 5,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 14,\n",
       " 13,\n",
       " 21,\n",
       " 13,\n",
       " 12,\n",
       " 15,\n",
       " 11,\n",
       " 18,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 14,\n",
       " 8,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 11,\n",
       " 9,\n",
       " 12,\n",
       " 18,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 15,\n",
       " 9,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 21,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 21,\n",
       " 15,\n",
       " 23,\n",
       " 14,\n",
       " 19,\n",
       " 18,\n",
       " 16,\n",
       " 12,\n",
       " 14,\n",
       " 10,\n",
       " 13,\n",
       " 17,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 8,\n",
       " 15,\n",
       " 12,\n",
       " 8,\n",
       " 10,\n",
       " 17,\n",
       " 19,\n",
       " 17,\n",
       " 19,\n",
       " 20,\n",
       " 15,\n",
       " 18,\n",
       " 14,\n",
       " 8,\n",
       " 9,\n",
       " 16,\n",
       " 15,\n",
       " 12,\n",
       " 15,\n",
       " 11,\n",
       " 12,\n",
       " 18,\n",
       " 14,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 11,\n",
       " 12,\n",
       " 15,\n",
       " 13,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 14,\n",
       " 12,\n",
       " 13,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 13,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 11,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 12,\n",
       " 15,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 19,\n",
       " 17,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 5,\n",
       " 16,\n",
       " 17,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 18,\n",
       " 7,\n",
       " 4,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 14,\n",
       " 16,\n",
       " 15,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 13,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 19,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 16,\n",
       " 19,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 15,\n",
       " 12,\n",
       " 12,\n",
       " 16,\n",
       " 11,\n",
       " 14,\n",
       " 15,\n",
       " 13,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 16,\n",
       " 13,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 17,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 9,\n",
       " 12,\n",
       " 27,\n",
       " 20,\n",
       " 19,\n",
       " 25,\n",
       " 21,\n",
       " 24,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 16,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 9,\n",
       " 12,\n",
       " 15,\n",
       " 8,\n",
       " 19,\n",
       " 19,\n",
       " 23,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 10,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 12,\n",
       " 10,\n",
       " 17,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 15,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 19,\n",
       " 16,\n",
       " 17,\n",
       " 20,\n",
       " 18,\n",
       " 16,\n",
       " 21,\n",
       " 20,\n",
       " 17,\n",
       " 17,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 16,\n",
       " 18,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 11,\n",
       " 18,\n",
       " 12,\n",
       " 7,\n",
       " 21,\n",
       " 16,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 17,\n",
       " 11,\n",
       " 8,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 10,\n",
       " 7,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 13,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 14,\n",
       " 12,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 13,\n",
       " 9,\n",
       " 8,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 14,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 15,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 15,\n",
       " 19,\n",
       " 17,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 14,\n",
       " 11,\n",
       " 9,\n",
       " 7,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 22,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 19,\n",
       " 9,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 19,\n",
       " 13,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 13,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 15,\n",
       " 7,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 15,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 16,\n",
       " 15,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 15,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 21,\n",
       " 17,\n",
       " 23,\n",
       " 10,\n",
       " 13,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 11,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 16,\n",
       " 12,\n",
       " 17,\n",
       " 17,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 14,\n",
       " 17,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 9,\n",
       " 8,\n",
       " 15,\n",
       " 14,\n",
       " 10,\n",
       " 14,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 11,\n",
       " 13,\n",
       " 11,\n",
       " 7,\n",
       " 15,\n",
       " 10,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_dens = pd.read_csv('data/cell_density_10.csv')['Value'].tolist()\n",
    "cell_dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7c4a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 441,
       "width": 709
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cell_dens, kde=True, bins=15, color='blue')\n",
    "plt.title('Distribution of Cell Density', fontsize=16)\n",
    "plt.xlabel('Cell Density', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d312022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def binning(cell_density, number_bins):\n",
    "    \"\"\"\n",
    "    Assigns each cell density value to an integer bin starting from 1.\n",
    "\n",
    "    Parameters:\n",
    "    - cell_density (list or np.array): List/array of cell density values.\n",
    "    - number_bins (int): Number of bins to divide the data into.\n",
    "\n",
    "    Returns:\n",
    "    - new_cell_density (np.array): Array of bin indices (1-based).\n",
    "    \"\"\"\n",
    "    # Compute the min and max of the cell densities\n",
    "    max_val = np.max(cell_density)\n",
    "    min_val = np.min(cell_density)\n",
    "    \n",
    "    # Generate bin edges\n",
    "    bins = np.linspace(min_val, max_val, number_bins + 1)  # Create number_bins intervals\n",
    "    \n",
    "    # Reassign cell densities to bins\n",
    "    new_cell_density = np.digitize(cell_density, bins, right=False)  # Bin indices start from 1 by default\n",
    "\n",
    "    # Ensure all values fall into valid bins (adjust if max value is included)\n",
    "    new_cell_density = np.clip(new_cell_density, 1, number_bins)\n",
    "\n",
    "    return new_cell_density\n",
    "\n",
    "\n",
    "new_cell_dens=binning(cell_dens,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ba146b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>vertex_x</th>\n",
       "      <th>vertex_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>200.17500</td>\n",
       "      <td>1813.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>199.32501</td>\n",
       "      <td>1814.5376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>199.32501</td>\n",
       "      <td>1815.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>199.11250</td>\n",
       "      <td>1815.3876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>199.32501</td>\n",
       "      <td>1815.6001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085834</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.45000</td>\n",
       "      <td>1527.2375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085835</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.23800</td>\n",
       "      <td>1527.0250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085836</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.23800</td>\n",
       "      <td>1526.6001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085837</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.02500</td>\n",
       "      <td>1526.3876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085838</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4620.17530</td>\n",
       "      <td>1526.3876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4085839 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cell_id    vertex_x   vertex_y\n",
       "0        aaaaficg-1   200.17500  1813.6875\n",
       "1        aaaaficg-1   199.32501  1814.5376\n",
       "2        aaaaficg-1   199.32501  1815.1750\n",
       "3        aaaaficg-1   199.11250  1815.3876\n",
       "4        aaaaficg-1   199.32501  1815.6001\n",
       "...             ...         ...        ...\n",
       "4085834  ojacpeii-1  4621.45000  1527.2375\n",
       "4085835  ojacpeii-1  4621.23800  1527.0250\n",
       "4085836  ojacpeii-1  4621.23800  1526.6001\n",
       "4085837  ojacpeii-1  4621.02500  1526.3876\n",
       "4085838  ojacpeii-1  4620.17530  1526.3876\n",
       "\n",
       "[4085839 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nucleus_boundaries = pd.read_csv('data/nucleus_boundaries.csv.gz')\n",
    "nucleus_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94af7329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>vertex_x</th>\n",
       "      <th>vertex_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaadpbp-1</td>\n",
       "      <td>205.06250</td>\n",
       "      <td>1489.8375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaadpbp-1</td>\n",
       "      <td>204.21251</td>\n",
       "      <td>1490.4751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaadpbp-1</td>\n",
       "      <td>204.00000</td>\n",
       "      <td>1491.1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaadpbp-1</td>\n",
       "      <td>202.08751</td>\n",
       "      <td>1493.4501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaadpbp-1</td>\n",
       "      <td>201.87500</td>\n",
       "      <td>1494.7251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054154</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.45000</td>\n",
       "      <td>1527.2375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054155</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.23800</td>\n",
       "      <td>1527.0250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054156</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.23800</td>\n",
       "      <td>1526.6001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054157</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4621.02500</td>\n",
       "      <td>1526.3876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054158</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>4620.17530</td>\n",
       "      <td>1526.3876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4054159 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cell_id    vertex_x   vertex_y\n",
       "0        aaaadpbp-1   205.06250  1489.8375\n",
       "1        aaaadpbp-1   204.21251  1490.4751\n",
       "2        aaaadpbp-1   204.00000  1491.1125\n",
       "3        aaaadpbp-1   202.08751  1493.4501\n",
       "4        aaaadpbp-1   201.87500  1494.7251\n",
       "...             ...         ...        ...\n",
       "4054154  ojacpeii-1  4621.45000  1527.2375\n",
       "4054155  ojacpeii-1  4621.23800  1527.0250\n",
       "4054156  ojacpeii-1  4621.23800  1526.6001\n",
       "4054157  ojacpeii-1  4621.02500  1526.3876\n",
       "4054158  ojacpeii-1  4620.17530  1526.3876\n",
       "\n",
       "[4054159 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cell_boundaries = pd.read_csv('data/cell_boundaries.csv.gz')\n",
    "cell_boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2248c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>soma_joinid</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENSG00000121410</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ENSG00000268895</td>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>3374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ENSG00000175899</td>\n",
       "      <td>A2M</td>\n",
       "      <td>6318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ENSG00000245105</td>\n",
       "      <td>A2M-AS1</td>\n",
       "      <td>2948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60659</th>\n",
       "      <td>60659</td>\n",
       "      <td>60659</td>\n",
       "      <td>ENSG00000288719</td>\n",
       "      <td>RP4-669P10.21</td>\n",
       "      <td>4252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60660</th>\n",
       "      <td>60660</td>\n",
       "      <td>60660</td>\n",
       "      <td>ENSG00000288720</td>\n",
       "      <td>RP11-852E15.3</td>\n",
       "      <td>7007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60661</th>\n",
       "      <td>60661</td>\n",
       "      <td>60661</td>\n",
       "      <td>ENSG00000288721</td>\n",
       "      <td>RP5-973N23.5</td>\n",
       "      <td>7765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60662</th>\n",
       "      <td>60662</td>\n",
       "      <td>60662</td>\n",
       "      <td>ENSG00000288723</td>\n",
       "      <td>RP11-553N16.6</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60663</th>\n",
       "      <td>60663</td>\n",
       "      <td>60663</td>\n",
       "      <td>ENSG00000288724</td>\n",
       "      <td>RP13-546I2.2</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60664 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  soma_joinid       feature_id   feature_name  feature_length\n",
       "0               0            0  ENSG00000121410           A1BG            3999\n",
       "1               1            1  ENSG00000268895       A1BG-AS1            3374\n",
       "2               2            2  ENSG00000148584           A1CF            9603\n",
       "3               3            3  ENSG00000175899            A2M            6318\n",
       "4               4            4  ENSG00000245105        A2M-AS1            2948\n",
       "...           ...          ...              ...            ...             ...\n",
       "60659       60659        60659  ENSG00000288719  RP4-669P10.21            4252\n",
       "60660       60660        60660  ENSG00000288720  RP11-852E15.3            7007\n",
       "60661       60661        60661  ENSG00000288721   RP5-973N23.5            7765\n",
       "60662       60662        60662  ENSG00000288723  RP11-553N16.6            1015\n",
       "60663       60663        60663  ENSG00000288724   RP13-546I2.2             625\n",
       "\n",
       "[60664 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_info = pd.read_csv('data/gene_info.csv')\n",
    "index_column = gene_info['feature_id']\n",
    "gene_names_list = gene_info['feature_name'].tolist()\n",
    "gene_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99814977",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_csv('data/transcripts.csv')\n",
    "filtered_transcripts = transcripts[transcripts['cell_id'] != 'UNASSIGNED']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c847673",
   "metadata": {},
   "source": [
    "## Formating of Xenium Dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acefa5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_centroid</th>\n",
       "      <th>y_centroid</th>\n",
       "      <th>transcript_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>cell_area</th>\n",
       "      <th>nucleus_area</th>\n",
       "      <th>ABCC11</th>\n",
       "      <th>ACE2</th>\n",
       "      <th>ACKR1</th>\n",
       "      <th>ACTA2</th>\n",
       "      <th>...</th>\n",
       "      <th>TSPAN19</th>\n",
       "      <th>UBE2C</th>\n",
       "      <th>UMOD</th>\n",
       "      <th>UPK3B</th>\n",
       "      <th>VCAN</th>\n",
       "      <th>VSIG4</th>\n",
       "      <th>VWA5A</th>\n",
       "      <th>VWF</th>\n",
       "      <th>cell_boundaries</th>\n",
       "      <th>nucleus_boundaries</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaadpbp-1</th>\n",
       "      <td>206.089813</td>\n",
       "      <td>1495.898193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.456877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[205.0625, 1489.8375], [204.21251, 1490.4751]...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaficg-1</th>\n",
       "      <td>201.765823</td>\n",
       "      <td>1816.210815</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>49.130002</td>\n",
       "      <td>21.268595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[200.8125, 1811.9875], [199.5375, 1812.625], ...</td>\n",
       "      <td>[[200.175, 1813.6875], [199.32501, 1814.5376],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaabbaka-1</th>\n",
       "      <td>179.024506</td>\n",
       "      <td>2167.253906</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>119.618911</td>\n",
       "      <td>74.778753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[177.01251, 2158.575], [175.52501, 2159.425],...</td>\n",
       "      <td>[[177.225, 2162.4001], [176.8, 2162.6125], [17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaabbjoo-1</th>\n",
       "      <td>186.060654</td>\n",
       "      <td>2163.309082</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>94.241097</td>\n",
       "      <td>59.109533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[185.51251, 2156.2375], [184.2375, 2157.3], [...</td>\n",
       "      <td>[[185.9375, 2156.875], [184.875, 2157.3], [183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaablchg-1</th>\n",
       "      <td>200.246887</td>\n",
       "      <td>2198.593506</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>120.341411</td>\n",
       "      <td>52.426408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[197.20001, 2191.9375], [197.4125, 2192.1501]...</td>\n",
       "      <td>[[197.4125, 2193.425], [197.4125, 2193.6375], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ojaaphhh-1</th>\n",
       "      <td>4552.125977</td>\n",
       "      <td>1643.896484</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26.913126</td>\n",
       "      <td>11.153594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[4551.5376, 1640.925], [4551.325, 1641.1376],...</td>\n",
       "      <td>[[4551.5376, 1641.9875], [4551.325, 1642.2001]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ojabeldf-1</th>\n",
       "      <td>4437.434082</td>\n",
       "      <td>1629.141846</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5.418750</td>\n",
       "      <td>5.418750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[4437.2124, 1627.75], [4437.0, 1627.9625], [4...</td>\n",
       "      <td>[[4437.2124, 1627.75], [4437.0, 1627.9625], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ojacfbid-1</th>\n",
       "      <td>4463.312988</td>\n",
       "      <td>1576.604004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.502500</td>\n",
       "      <td>6.502500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[4462.7124, 1575.05], [4462.2876, 1575.4751],...</td>\n",
       "      <td>[[4462.7124, 1575.05], [4462.2876, 1575.4751],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ojacfhhg-1</th>\n",
       "      <td>4619.915527</td>\n",
       "      <td>1454.322388</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.089532</td>\n",
       "      <td>7.089532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[4619.9624, 1452.8625], [4619.75, 1453.0751],...</td>\n",
       "      <td>[[4619.9624, 1452.8625], [4619.75, 1453.0751],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ojacpeii-1</th>\n",
       "      <td>4620.281250</td>\n",
       "      <td>1528.107666</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.683125</td>\n",
       "      <td>6.683125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[4620.1753, 1526.3876], [4619.75, 1526.8125],...</td>\n",
       "      <td>[[4620.1753, 1526.3876], [4619.75, 1526.8125],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162254 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_centroid   y_centroid  transcript_counts  total_counts  \\\n",
       "cell_id                                                                 \n",
       "aaaadpbp-1   206.089813  1495.898193                  0             0   \n",
       "aaaaficg-1   201.765823  1816.210815                 19            19   \n",
       "aaabbaka-1   179.024506  2167.253906                 53            53   \n",
       "aaabbjoo-1   186.060654  2163.309082                 29            29   \n",
       "aaablchg-1   200.246887  2198.593506                 42            42   \n",
       "...                 ...          ...                ...           ...   \n",
       "ojaaphhh-1  4552.125977  1643.896484                  3             3   \n",
       "ojabeldf-1  4437.434082  1629.141846                  6             6   \n",
       "ojacfbid-1  4463.312988  1576.604004                  0             0   \n",
       "ojacfhhg-1  4619.915527  1454.322388                  2             2   \n",
       "ojacpeii-1  4620.281250  1528.107666                  1             1   \n",
       "\n",
       "             cell_area  nucleus_area  ABCC11  ACE2  ACKR1  ACTA2  ...  \\\n",
       "cell_id                                                           ...   \n",
       "aaaadpbp-1   68.456877      0.000000     0.0   0.0    0.0    0.0  ...   \n",
       "aaaaficg-1   49.130002     21.268595     0.0   0.0    0.0    0.0  ...   \n",
       "aaabbaka-1  119.618911     74.778753     0.0   0.0    0.0    0.0  ...   \n",
       "aaabbjoo-1   94.241097     59.109533     0.0   0.0    0.0    0.0  ...   \n",
       "aaablchg-1  120.341411     52.426408     0.0   0.0    0.0    0.0  ...   \n",
       "...                ...           ...     ...   ...    ...    ...  ...   \n",
       "ojaaphhh-1   26.913126     11.153594     0.0   0.0    0.0    0.0  ...   \n",
       "ojabeldf-1    5.418750      5.418750     0.0   0.0    0.0    0.0  ...   \n",
       "ojacfbid-1    6.502500      6.502500     0.0   0.0    0.0    0.0  ...   \n",
       "ojacfhhg-1    7.089532      7.089532     0.0   0.0    0.0    0.0  ...   \n",
       "ojacpeii-1    6.683125      6.683125     0.0   0.0    0.0    0.0  ...   \n",
       "\n",
       "            TSPAN19  UBE2C  UMOD  UPK3B  VCAN  VSIG4  VWA5A  VWF  \\\n",
       "cell_id                                                            \n",
       "aaaadpbp-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "aaaaficg-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "aaabbaka-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "aaabbjoo-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "aaablchg-1      0.0    0.0   0.0    0.0   0.0    1.0    0.0  1.0   \n",
       "...             ...    ...   ...    ...   ...    ...    ...  ...   \n",
       "ojaaphhh-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "ojabeldf-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "ojacfbid-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "ojacfhhg-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "ojacpeii-1      0.0    0.0   0.0    0.0   0.0    0.0    0.0  0.0   \n",
       "\n",
       "                                              cell_boundaries  \\\n",
       "cell_id                                                         \n",
       "aaaadpbp-1  [[205.0625, 1489.8375], [204.21251, 1490.4751]...   \n",
       "aaaaficg-1  [[200.8125, 1811.9875], [199.5375, 1812.625], ...   \n",
       "aaabbaka-1  [[177.01251, 2158.575], [175.52501, 2159.425],...   \n",
       "aaabbjoo-1  [[185.51251, 2156.2375], [184.2375, 2157.3], [...   \n",
       "aaablchg-1  [[197.20001, 2191.9375], [197.4125, 2192.1501]...   \n",
       "...                                                       ...   \n",
       "ojaaphhh-1  [[4551.5376, 1640.925], [4551.325, 1641.1376],...   \n",
       "ojabeldf-1  [[4437.2124, 1627.75], [4437.0, 1627.9625], [4...   \n",
       "ojacfbid-1  [[4462.7124, 1575.05], [4462.2876, 1575.4751],...   \n",
       "ojacfhhg-1  [[4619.9624, 1452.8625], [4619.75, 1453.0751],...   \n",
       "ojacpeii-1  [[4620.1753, 1526.3876], [4619.75, 1526.8125],...   \n",
       "\n",
       "                                           nucleus_boundaries  \n",
       "cell_id                                                        \n",
       "aaaadpbp-1                                                NaN  \n",
       "aaaaficg-1  [[200.175, 1813.6875], [199.32501, 1814.5376],...  \n",
       "aaabbaka-1  [[177.225, 2162.4001], [176.8, 2162.6125], [17...  \n",
       "aaabbjoo-1  [[185.9375, 2156.875], [184.875, 2157.3], [183...  \n",
       "aaablchg-1  [[197.4125, 2193.425], [197.4125, 2193.6375], ...  \n",
       "...                                                       ...  \n",
       "ojaaphhh-1  [[4551.5376, 1641.9875], [4551.325, 1642.2001]...  \n",
       "ojabeldf-1  [[4437.2124, 1627.75], [4437.0, 1627.9625], [4...  \n",
       "ojacfbid-1  [[4462.7124, 1575.05], [4462.2876, 1575.4751],...  \n",
       "ojacfhhg-1  [[4619.9624, 1452.8625], [4619.75, 1453.0751],...  \n",
       "ojacpeii-1  [[4620.1753, 1526.3876], [4619.75, 1526.8125],...  \n",
       "\n",
       "[162254 rows x 385 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Annotating cell metadata\n",
    "cells_metadata = cells.set_index(\"cell_id\")\n",
    "\n",
    "# Example: Annotating gene metadata\n",
    "gene_metadata = gene_info.set_index(\"feature_name\")\n",
    "\n",
    "# Load cell boundaries and group by cell_id\n",
    "cell_boundaries_grouped = cell_boundaries.groupby(\"cell_id\")[[\"vertex_x\", \"vertex_y\"]].apply(lambda df: df.values.tolist())\n",
    "\n",
    "# Load nucleus boundaries similarly\n",
    "nucleus_boundaries_grouped = nucleus_boundaries.groupby(\"cell_id\")[[\"vertex_x\", \"vertex_y\"]].apply(lambda df: df.values.tolist())\n",
    "\n",
    "# Aggregate transcript counts per cell and feature\n",
    "transcript_counts = (\n",
    "    filtered_transcripts.groupby([\"cell_id\", \"feature_name\"]).size().unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Combine cells metadata with transcript counts\n",
    "merged_data = cells_metadata.join(transcript_counts, how=\"left\").fillna(0)\n",
    "\n",
    "# Add boundaries if needed\n",
    "merged_data[\"cell_boundaries\"] = cell_boundaries_grouped\n",
    "merged_data[\"nucleus_boundaries\"] = nucleus_boundaries_grouped\n",
    "#Filtering columns that are Unassigned ( and cells that have 0 transcripts )\n",
    "merged_data = merged_data.loc[:, ~merged_data.columns.str.contains(\"UnassignedCodeword\")]\n",
    "merged_data = merged_data.loc[:, ~merged_data.columns.str.contains(\"NegControlCodeword\")]\n",
    "merged_data = merged_data.loc[:, ~merged_data.columns.str.contains(\"NegControlProbe\")]\n",
    "\n",
    "# Subtract unassigned_codeword_counts from total_counts only where unassigned_codeword_counts is non-zero\n",
    "merged_data['total_counts'] -= merged_data['unassigned_codeword_counts'] * (merged_data['unassigned_codeword_counts'] != 0)\n",
    "\n",
    "# Subtract control_codeword_counts from total_counts only where control_codeword_counts is non-zero\n",
    "merged_data['total_counts'] -= merged_data['control_codeword_counts'] * (merged_data['control_codeword_counts'] != 0)\n",
    "\n",
    "# Subtract deprecated_codeword_counts from total_counts only where control_codeword_counts is non-zero\n",
    "\n",
    "#merged_data['total_counts'] -= merged_data['deprecated_codeword_counts'] * (merged_data['deprecated_codeword_counts'] != 0)\n",
    "\n",
    "# Subtract control_probe_counts from total_counts only where control_codeword_counts is non-zero\n",
    "\n",
    "merged_data['total_counts'] -= merged_data['control_probe_counts'] * (merged_data['control_probe_counts'] != 0)\n",
    "\n",
    "merged_data.drop('control_codeword_counts', axis=1, inplace=True)\n",
    "\n",
    "merged_data.drop('unassigned_codeword_counts', axis=1, inplace=True)\n",
    "\n",
    "merged_data.drop('deprecated_codeword_counts', axis=1, inplace=True)\n",
    "\n",
    "merged_data.drop('control_probe_counts', axis=1, inplace=True)\n",
    "\n",
    "# Put the correct type for each column\n",
    "\"\"\"\n",
    "merged_data['cell_id'] = merged_data['cell_id'].astype('category')\n",
    "\n",
    "merged_data['feature_name'] = merged_data['feature_name'].astype('category')\n",
    "merged_data['total_counts'] = merged_data['total_counts'].astype(int)\n",
    "merged_data['transcript_counts'] = merged_data['transcript_counts'].astype(int)\n",
    "merged_data.iloc[:, 6:-2]=merged_data.iloc[:, 6:-2].astype(int)\n",
    "\n",
    "\n",
    "merged_data['x_centroid'] = merged_data['x_centroid'].astype(float)\n",
    "merged_data['y_centroid'] = merged_data['y_centroid'].astype(float)\n",
    "merged_data['cell_area'] = merged_data['cell_area'].astype(float)\n",
    "merged_data['nucleus_area'] = merged_data['nucleus_area'].astype(float)\"\"\"\n",
    "\n",
    "merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894ab747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AQP3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=merged_data.iloc[-1,6:-2]\n",
    "b=np.where(a==1)\n",
    "index_26 = a.index[26] \n",
    "test=a[26] # Retrieves the index label of the 26th element\n",
    "index_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9371c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([62405], dtype=int64),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_names = merged_data.index  # Convert the row index to a list\n",
    "a=np.where(row_names=='fjjlmnma-1')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac17688",
   "metadata": {},
   "source": [
    "### Creation of Adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d36039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 162254 × 377\n",
      "    obs: 'x_centroid', 'y_centroid', 'transcript_counts', 'total_counts', 'cell_area', 'nucleus_area', 'cell_boundaries', 'nucleus_boundaries', 'batch_id', 'celltype_id', 'cell_density', 'cell_ids'\n",
      "    var: 'index_column', 'gene_name', 'total_count'\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Define the gene expression matrix (X) as a sparse matrix\n",
    "gene_expression = sp.csr_matrix(merged_data.iloc[:, 6:-2].values)\n",
    "\n",
    "# Define the observation metadata (obs)\n",
    "obs_metadata = merged_data.drop(merged_data.columns[6:-2], axis=1)\n",
    "\n",
    "# Define variable metadata (var)\n",
    "var_metadata = pd.DataFrame(index=merged_data.iloc[:, 6:-2].columns)\n",
    "\n",
    "# Create the AnnData object\n",
    "adata_spatial = AnnData(\n",
    "    X=gene_expression,          # Gene expression matrix as sparse\n",
    "    obs=obs_metadata,           # Cell-level metadata\n",
    "    var=var_metadata            # Gene-level metadata\n",
    ")\n",
    "# Ensure index_column_list is a list (if not already)\n",
    "index_column_list = index_column.tolist()  \n",
    "\n",
    "# Assign the index_column_list to the 'index' column in adata_spatial.var\n",
    "# Create a mapping dictionary from gene_names_list to index_column_list\n",
    "gene_to_index_mapping = dict(zip(gene_names_list, index_column_list))\n",
    "\n",
    "# Map the 'index' column in adata_spatial.var using the mapping dictionary\n",
    "adata_spatial.var[\"index_column\"] = adata_spatial.var.index.map(gene_to_index_mapping)\n",
    "adata_spatial.var['gene_name'] = adata_spatial.var.index.tolist()\n",
    "\n",
    "gene_counts = merged_data.iloc[:, 6:-2].sum(axis=0)  # Sum the gene counts across all cells\n",
    "gene_names_list = adata_spatial.var.index.tolist()  # List of gene names in spatial_adata\n",
    "#gene_counts_ordered = gene_counts[gene_names_list].values\n",
    "adata_spatial.var['total_count'] = gene_counts.values\n",
    "adata_spatial.obs['batch_id'] = np.zeros(162254)\n",
    "adata_spatial.obs['celltype_id'] = np.ones(162254)                                          # To change\n",
    "adata_spatial.obs['cell_density'] = cell_dens\n",
    "adata_spatial.obs['cell_ids'] = merged_data.index                                        \n",
    "\n",
    "\n",
    "\n",
    "# Optional: Print summary of the AnnData object\n",
    "print(adata_spatial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5e520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(adata_spatial.X[62405,176])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53b8b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.load_model is not None:\n",
    "#     model_dir = Path(config.load_model)\n",
    "#     model_config_file = model_dir / \"args.json\"\n",
    "#     model_file = model_dir / \"best_model.pt\"\n",
    "#     vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "#     vocab = GeneVocab.from_file(vocab_file)\n",
    "#     shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "#     \"\"\"\n",
    "#     for s in special_tokens:\n",
    "#         if s not in vocab:\n",
    "#             vocab.append_token(s)\n",
    "#     \"\"\"\n",
    "#     adata_spatial.var[\"id_in_vocab\"] = [\n",
    "#         1 if gene in vocab else -1 for gene in adata_spatial.var[\"gene_name\"]\n",
    "#     ]\n",
    "#     gene_ids_in_vocab = np.array(adata_spatial.var[\"id_in_vocab\"])\n",
    "#     logger.info(\n",
    "#         f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "#         f\"in vocabulary of size {len(vocab)}.\"\n",
    "#     )\n",
    "#     adata_spatial = adata_spatial[:, adata_spatial.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "#     # model\n",
    "#     with open(model_config_file, \"r\") as f:\n",
    "#         model_configs = json.load(f)\n",
    "#     logger.info(\n",
    "#         f\"Resume model from {model_file}, the model args will override the \"\n",
    "#         f\"config {model_config_file}.\"\n",
    "#     )\n",
    "#     embsize = model_configs[\"embsize\"]\n",
    "#     nhead = model_configs[\"nheads\"]\n",
    "#     d_hid = model_configs[\"d_hid\"]\n",
    "#     nlayers = model_configs[\"nlayers\"]\n",
    "#     n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b89103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor(adata_spatial, batch_key=None)\n",
    "# input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "#     \"normed_raw\": \"X_normed\",\n",
    "#     \"log1p\": \"X_normed\",\n",
    "#     \"binned\": \"X_binned\",\n",
    "# }[input_style]\n",
    "# all_counts = (\n",
    "#     adata_spatial.layers[input_layer_key].A\n",
    "#     if issparse(adata_spatial.layers[input_layer_key])\n",
    "#     else adata_spatial.layers[input_layer_key]\n",
    "# )\n",
    "# genes = adata_spatial.var[\"gene_name\"].tolist()\n",
    "\n",
    "# cell_density = adata_spatial.obs[\"cell_density\"].tolist()  # make sure count from 0                               # TO CHANGE FOR CELL DENSITY\n",
    "# cell_density = np.array(cell_density)                                                                    # TO CHANGE FOR CELL DENSITY\n",
    "\n",
    "# batch_ids = adata_spatial.obs[\"batch_id\"].tolist()\n",
    "# num_batch_types = len(set(batch_ids))\n",
    "# batch_ids = np.array(batch_ids)\n",
    "\n",
    "# (\n",
    "#     train_data, \n",
    "#     valid_data,\n",
    "#     train_cell_density,                                                                                           # TO CHANGE FOR CELL DENSITY            \n",
    "#     valid_cell_density,                                                                                            # TO CHANGE FOR CELL DENSITY     \n",
    "#     train_batch_labels,\n",
    "#     valid_batch_labels,\n",
    "# ) = train_test_split(\n",
    "#     all_counts, cell_density, batch_ids, test_size=0.4, shuffle=True                                        # TO CHANGE FOR CELL DENSITY   for this page change celltypes_labels into cell density\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b5d67",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for cell-type annotation task\n",
    "Listed below are some hyper-parameter recommendations for the cell-type task. Note that the CLS objective is on to facilitate cell-type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d07b5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"ms\",\n",
    "    do_train=True,\n",
    "    load_model=\"../save/scGPT_human\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=8e-3,\n",
    "    batch_size=32/4,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # 4number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c08ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexander-aujesky\u001b[0m (\u001b[33malexander-aujesky-epfl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Alexander\\Desktop\\ML\\scGPT\\tutorials\\wandb\\run-20241218_161605-gvixbn93</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-aujesky-epfl/scGPT/runs/gvixbn93' target=\"_blank\">mild-hill-79</a></strong> to <a href='https://wandb.ai/alexander-aujesky-epfl/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-aujesky-epfl/scGPT' target=\"_blank\">https://wandb.ai/alexander-aujesky-epfl/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-aujesky-epfl/scGPT/runs/gvixbn93' target=\"_blank\">https://wandb.ai/alexander-aujesky-epfl/scGPT/runs/gvixbn93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset_name': 'ms', 'do_train': True, 'load_model': '../save/scGPT_human', 'mask_ratio': 0.0, 'epochs': 10, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.008, 'batch_size': 8.0, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Set the start method to 'spawn' for Windows\n",
    "if mp.get_start_method() != 'spawn':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Initialize wandb with appropriate settings\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"spawn\"),  # Use 'spawn' on Windows\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"log1p\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"normed_raw\"  # \"normed_raw\", \"log1p\", or \"binned\"                                                                                      # Is binned relevant for Cell density ? \n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.                                                                 #Consider usefulness for Cell density\n",
    "CLS = False  # celltype classification objective  \n",
    "CDP = True   \n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"w-pool\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr = 1e-4\n",
    "lr_ADV = 1e-2  # learning rate for discriminator, used when ADV is True\n",
    "batch_size =  128 # config.batch_size\n",
    "\n",
    "eval_batch_size =int( config.batch_size)\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size/2  # embedding  \n",
    "embsize = 32\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "d_hid = 4\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nlayers = 4\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True                                                                                             #Might need to change evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17ff2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf7112d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save\\dev_ms-Dec18-16-16\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc7002",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data\n",
    "We follow the standard scGPT data pre-processing pipelines for the cell-type annotation task. Note that since now we have two datasets at hand (i.e., reference and query data), the same pre-prpocessing steps need to be applied to both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95b50200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"ms\":\n",
    "    data_dir = Path(\"../data/ms\")\n",
    "    adata = sc.read(data_dir / \"c_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"filtered_ms_adata.h5ad\")\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")                                               # Load for cell density\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "                \n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc5a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 376/377 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ..\\save\\scGPT_human\\best_model.pt, the model args will override the config ..\\save\\scGPT_human\\args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata_spatial.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata_spatial.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata_spatial.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata_spatial = adata_spatial[:, adata_spatial.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    embsize = 128\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]-500\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    nlayers = 16\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d08757ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=True,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=0,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "#adata_test = adata_spatial[adata_spatial.obs[\"str_batch\"] == \"1\"]\n",
    "#adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata_spatial, batch_key=None)\n",
    "#preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d8d658f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 161993 × 376\n",
       "    obs: 'x_centroid', 'y_centroid', 'transcript_counts', 'total_counts', 'cell_area', 'nucleus_area', 'cell_boundaries', 'nucleus_boundaries', 'batch_id', 'celltype_id', 'cell_density', 'cell_ids', 'n_counts'\n",
       "    var: 'index_column', 'gene_name', 'total_count', 'id_in_vocab'\n",
       "    layers: 'X_normed'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbc1b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata_spatial.layers[input_layer_key].A\n",
    "    if issparse(adata_spatial.layers[input_layer_key])\n",
    "    else adata_spatial.layers[input_layer_key]\n",
    ")\n",
    "genes = adata_spatial.var[\"gene_name\"].tolist()\n",
    "\n",
    "cell_density = adata_spatial.obs[\"cell_density\"].values.astype(float)  # Continuous target variable\n",
    "  # make sure count from 0                               # TO CHANGE FOR CELL DENSITY\n",
    "cell_density = np.array(cell_density)                                                                    # TO CHANGE FOR CELL DENSITY\n",
    "\n",
    "batch_ids = adata_spatial.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data, \n",
    "    valid_data,\n",
    "    train_cell_density,                                                                                           # TO CHANGE FOR CELL DENSITY            \n",
    "    valid_cell_density,                                                                                            # TO CHANGE FOR CELL DENSITY     \n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, cell_density, batch_ids, test_size=0.1, shuffle=True                                        # TO CHANGE FOR CELL DENSITY   for this page change celltypes_labels into cell density\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cd701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "818bfcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 145793, \n",
      "\t feature length: 132\n",
      "scGPT - INFO - valid set number of samples: 16200, \n",
      "\t feature length: 123\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37a80818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    # Masked values for train and validation datasets\n",
    "    \n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "    \n",
    "    # Gene IDs and values\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    # Replace celltype labels with cell density\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Replace with cell density\n",
    "    tensor_cell_density_train = torch.from_numpy(train_cell_density).float()\n",
    "    tensor_cell_density_valid = torch.from_numpy(valid_cell_density).float()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        # Sorting for training\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_cell_density_train = tensor_cell_density_train[train_sort_ids]\n",
    "\n",
    "        # Sorting for validation\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_cell_density_valid = tensor_cell_density_valid[valid_sort_ids]\n",
    "\n",
    "    # Prepare training and validation datasets\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"cell_density\": tensor_cell_density_train,  # Updated label\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"cell_density\": tensor_cell_density_valid,  # Updated label\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return {k: v[idx] for k, v in self.data.items()}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "# data_\n",
    "def prepare_dataloader(\n",
    "    \n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(multiprocessing.cpu_count(), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "    \n",
    "        subsets = []\n",
    "        \"\"\"batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\"\"\"\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 0,  # Set to 0 or a small number\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,  # Force single-process loading\n",
    "        pin_memory=False\n",
    "    )\n",
    "    return data_loader\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105fda",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82b77b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - ERROR - Error loading model from ..\\save\\scGPT_human\\best_model.pt: Error(s) in loading state_dict for TransformerModel:\n",
      "\tMissing key(s) in state_dict: \"transformer_encoder.layers.0.self_attn.in_proj_weight\", \"transformer_encoder.layers.0.self_attn.in_proj_bias\", \"transformer_encoder.layers.1.self_attn.in_proj_weight\", \"transformer_encoder.layers.1.self_attn.in_proj_bias\", \"transformer_encoder.layers.2.self_attn.in_proj_weight\", \"transformer_encoder.layers.2.self_attn.in_proj_bias\", \"transformer_encoder.layers.3.self_attn.in_proj_weight\", \"transformer_encoder.layers.3.self_attn.in_proj_bias\", \"transformer_encoder.layers.4.self_attn.in_proj_weight\", \"transformer_encoder.layers.4.self_attn.in_proj_bias\", \"transformer_encoder.layers.5.self_attn.in_proj_weight\", \"transformer_encoder.layers.5.self_attn.in_proj_bias\", \"transformer_encoder.layers.6.self_attn.in_proj_weight\", \"transformer_encoder.layers.6.self_attn.in_proj_bias\", \"transformer_encoder.layers.7.self_attn.in_proj_weight\", \"transformer_encoder.layers.7.self_attn.in_proj_bias\", \"transformer_encoder.layers.8.self_attn.in_proj_weight\", \"transformer_encoder.layers.8.self_attn.in_proj_bias\", \"transformer_encoder.layers.9.self_attn.in_proj_weight\", \"transformer_encoder.layers.9.self_attn.in_proj_bias\", \"transformer_encoder.layers.10.self_attn.in_proj_weight\", \"transformer_encoder.layers.10.self_attn.in_proj_bias\", \"transformer_encoder.layers.11.self_attn.in_proj_weight\", \"transformer_encoder.layers.11.self_attn.in_proj_bias\", \"transformer_encoder.layers.12.self_attn.in_proj_weight\", \"transformer_encoder.layers.12.self_attn.in_proj_bias\", \"transformer_encoder.layers.12.self_attn.out_proj.weight\", \"transformer_encoder.layers.12.self_attn.out_proj.bias\", \"transformer_encoder.layers.12.linear1.weight\", \"transformer_encoder.layers.12.linear1.bias\", \"transformer_encoder.layers.12.linear2.weight\", \"transformer_encoder.layers.12.linear2.bias\", \"transformer_encoder.layers.12.norm1.weight\", \"transformer_encoder.layers.12.norm1.bias\", \"transformer_encoder.layers.12.norm2.weight\", \"transformer_encoder.layers.12.norm2.bias\", \"transformer_encoder.layers.13.self_attn.in_proj_weight\", \"transformer_encoder.layers.13.self_attn.in_proj_bias\", \"transformer_encoder.layers.13.self_attn.out_proj.weight\", \"transformer_encoder.layers.13.self_attn.out_proj.bias\", \"transformer_encoder.layers.13.linear1.weight\", \"transformer_encoder.layers.13.linear1.bias\", \"transformer_encoder.layers.13.linear2.weight\", \"transformer_encoder.layers.13.linear2.bias\", \"transformer_encoder.layers.13.norm1.weight\", \"transformer_encoder.layers.13.norm1.bias\", \"transformer_encoder.layers.13.norm2.weight\", \"transformer_encoder.layers.13.norm2.bias\", \"transformer_encoder.layers.14.self_attn.in_proj_weight\", \"transformer_encoder.layers.14.self_attn.in_proj_bias\", \"transformer_encoder.layers.14.self_attn.out_proj.weight\", \"transformer_encoder.layers.14.self_attn.out_proj.bias\", \"transformer_encoder.layers.14.linear1.weight\", \"transformer_encoder.layers.14.linear1.bias\", \"transformer_encoder.layers.14.linear2.weight\", \"transformer_encoder.layers.14.linear2.bias\", \"transformer_encoder.layers.14.norm1.weight\", \"transformer_encoder.layers.14.norm1.bias\", \"transformer_encoder.layers.14.norm2.weight\", \"transformer_encoder.layers.14.norm2.bias\", \"transformer_encoder.layers.15.self_attn.in_proj_weight\", \"transformer_encoder.layers.15.self_attn.in_proj_bias\", \"transformer_encoder.layers.15.self_attn.out_proj.weight\", \"transformer_encoder.layers.15.self_attn.out_proj.bias\", \"transformer_encoder.layers.15.linear1.weight\", \"transformer_encoder.layers.15.linear1.bias\", \"transformer_encoder.layers.15.linear2.weight\", \"transformer_encoder.layers.15.linear2.bias\", \"transformer_encoder.layers.15.norm1.weight\", \"transformer_encoder.layers.15.norm1.bias\", \"transformer_encoder.layers.15.norm2.weight\", \"transformer_encoder.layers.15.norm2.bias\", \"cls_decoder._decoder.0.weight\", \"cls_decoder._decoder.0.bias\", \"cls_decoder._decoder.2.weight\", \"cls_decoder._decoder.2.bias\", \"cls_decoder._decoder.3.weight\", \"cls_decoder._decoder.3.bias\", \"cls_decoder._decoder.5.weight\", \"cls_decoder._decoder.5.bias\", \"cls_decoder._decoder.6.weight\", \"cls_decoder._decoder.6.bias\", \"cls_decoder._decoder.8.weight\", \"cls_decoder._decoder.8.bias\", \"cls_decoder._decoder.9.weight\", \"cls_decoder._decoder.9.bias\", \"cls_decoder._decoder.11.weight\", \"cls_decoder._decoder.11.bias\", \"cls_decoder._decoder.12.weight\", \"cls_decoder._decoder.12.bias\", \"cls_decoder._decoder.14.weight\", \"cls_decoder._decoder.14.bias\", \"cls_decoder._decoder.15.weight\", \"cls_decoder._decoder.15.bias\", \"cls_decoder._decoder.17.weight\", \"cls_decoder._decoder.17.bias\", \"cls_decoder._decoder.18.weight\", \"cls_decoder._decoder.18.bias\", \"cls_decoder._decoder.20.weight\", \"cls_decoder._decoder.20.bias\", \"cls_decoder._decoder.21.weight\", \"cls_decoder._decoder.21.bias\", \"cls_decoder._decoder.23.weight\", \"cls_decoder._decoder.23.bias\", \"cls_decoder._decoder.24.weight\", \"cls_decoder._decoder.24.bias\", \"cls_decoder._decoder.26.weight\", \"cls_decoder._decoder.26.bias\", \"cls_decoder._decoder.27.weight\", \"cls_decoder._decoder.27.bias\", \"cls_decoder._decoder.29.weight\", \"cls_decoder._decoder.29.bias\", \"cls_decoder._decoder.30.weight\", \"cls_decoder._decoder.30.bias\", \"cls_decoder._decoder.32.weight\", \"cls_decoder._decoder.32.bias\", \"cls_decoder._decoder.33.weight\", \"cls_decoder._decoder.33.bias\", \"cls_decoder._decoder.35.weight\", \"cls_decoder._decoder.35.bias\", \"cls_decoder._decoder.36.weight\", \"cls_decoder._decoder.36.bias\", \"cls_decoder._decoder.38.weight\", \"cls_decoder._decoder.38.bias\", \"cls_decoder._decoder.39.weight\", \"cls_decoder._decoder.39.bias\", \"cls_decoder._decoder.41.weight\", \"cls_decoder._decoder.41.bias\", \"cls_decoder._decoder.42.weight\", \"cls_decoder._decoder.42.bias\", \"cls_decoder._decoder.44.weight\", \"cls_decoder._decoder.44.bias\", \"cls_decoder._decoder.45.weight\", \"cls_decoder._decoder.45.bias\", \"cls_decoder._decoder.47.weight\", \"cls_decoder._decoder.47.bias\", \"cls_decoder._decoder.48.weight\", \"cls_decoder._decoder.48.bias\", \"cls_decoder._decoder.50.weight\", \"cls_decoder._decoder.50.bias\", \"cls_decoder._decoder.51.weight\", \"cls_decoder._decoder.51.bias\", \"cls_decoder._decoder.53.weight\", \"cls_decoder._decoder.53.bias\", \"cls_decoder._decoder.54.weight\", \"cls_decoder._decoder.54.bias\", \"cls_decoder._decoder.56.weight\", \"cls_decoder._decoder.56.bias\", \"cls_decoder._decoder.57.weight\", \"cls_decoder._decoder.57.bias\", \"cls_decoder._decoder.59.weight\", \"cls_decoder._decoder.59.bias\", \"cls_decoder._decoder.60.weight\", \"cls_decoder._decoder.60.bias\", \"cls_decoder._decoder.62.weight\", \"cls_decoder._decoder.62.bias\", \"cls_decoder._decoder.63.weight\", \"cls_decoder._decoder.63.bias\", \"cls_decoder._decoder.65.weight\", \"cls_decoder._decoder.65.bias\", \"cls_decoder._decoder.66.weight\", \"cls_decoder._decoder.66.bias\", \"cls_decoder._decoder.68.weight\", \"cls_decoder._decoder.68.bias\", \"cls_decoder._decoder.69.weight\", \"cls_decoder._decoder.69.bias\", \"cls_decoder._decoder.71.weight\", \"cls_decoder._decoder.71.bias\", \"cls_decoder._decoder.72.weight\", \"cls_decoder._decoder.72.bias\", \"cls_decoder._decoder.74.weight\", \"cls_decoder._decoder.74.bias\", \"cls_decoder._decoder.75.weight\", \"cls_decoder._decoder.75.bias\", \"cls_decoder._decoder.77.weight\", \"cls_decoder._decoder.77.bias\", \"cls_decoder._decoder.78.weight\", \"cls_decoder._decoder.78.bias\", \"cls_decoder._decoder.80.weight\", \"cls_decoder._decoder.80.bias\", \"cls_decoder._decoder.81.weight\", \"cls_decoder._decoder.81.bias\", \"cls_decoder._decoder.83.weight\", \"cls_decoder._decoder.83.bias\", \"cls_decoder._decoder.84.weight\", \"cls_decoder._decoder.84.bias\", \"cls_decoder._decoder.86.weight\", \"cls_decoder._decoder.86.bias\", \"cls_decoder._decoder.87.weight\", \"cls_decoder._decoder.87.bias\", \"cls_decoder._decoder.89.weight\", \"cls_decoder._decoder.89.bias\", \"cls_decoder._decoder.90.weight\", \"cls_decoder._decoder.90.bias\", \"cls_decoder._decoder.92.weight\", \"cls_decoder._decoder.92.bias\", \"cls_decoder._decoder.93.weight\", \"cls_decoder._decoder.93.bias\", \"cls_decoder._decoder.95.weight\", \"cls_decoder._decoder.95.bias\", \"cls_decoder._decoder.96.weight\", \"cls_decoder._decoder.96.bias\", \"cls_decoder._decoder.98.weight\", \"cls_decoder._decoder.98.bias\", \"cls_decoder._decoder.99.weight\", \"cls_decoder._decoder.99.bias\", \"cls_decoder._decoder.101.weight\", \"cls_decoder._decoder.101.bias\", \"cls_decoder._decoder.102.weight\", \"cls_decoder._decoder.102.bias\", \"cls_decoder._decoder.104.weight\", \"cls_decoder._decoder.104.bias\", \"cls_decoder._decoder.105.weight\", \"cls_decoder._decoder.105.bias\", \"cls_decoder._decoder.107.weight\", \"cls_decoder._decoder.107.bias\", \"cls_decoder._decoder.108.weight\", \"cls_decoder._decoder.108.bias\", \"cls_decoder._decoder.110.weight\", \"cls_decoder._decoder.110.bias\", \"cls_decoder._decoder.111.weight\", \"cls_decoder._decoder.111.bias\", \"cls_decoder._decoder.113.weight\", \"cls_decoder._decoder.113.bias\", \"cls_decoder._decoder.114.weight\", \"cls_decoder._decoder.114.bias\", \"cls_decoder._decoder.116.weight\", \"cls_decoder._decoder.116.bias\", \"cls_decoder._decoder.117.weight\", \"cls_decoder._decoder.117.bias\", \"cls_decoder._decoder.119.weight\", \"cls_decoder._decoder.119.bias\", \"cls_decoder._decoder.120.weight\", \"cls_decoder._decoder.120.bias\", \"cls_decoder._decoder.122.weight\", \"cls_decoder._decoder.122.bias\", \"cls_decoder._decoder.123.weight\", \"cls_decoder._decoder.123.bias\", \"cls_decoder._decoder.125.weight\", \"cls_decoder._decoder.125.bias\", \"cls_decoder._decoder.126.weight\", \"cls_decoder._decoder.126.bias\", \"cls_decoder._decoder.128.weight\", \"cls_decoder._decoder.128.bias\", \"cls_decoder._decoder.129.weight\", \"cls_decoder._decoder.129.bias\", \"cls_decoder._decoder.131.weight\", \"cls_decoder._decoder.131.bias\", \"cls_decoder._decoder.132.weight\", \"cls_decoder._decoder.132.bias\", \"cls_decoder._decoder.134.weight\", \"cls_decoder._decoder.134.bias\", \"cls_decoder._decoder.135.weight\", \"cls_decoder._decoder.135.bias\", \"cls_decoder._decoder.137.weight\", \"cls_decoder._decoder.137.bias\", \"cls_decoder._decoder.138.weight\", \"cls_decoder._decoder.138.bias\", \"cls_decoder._decoder.140.weight\", \"cls_decoder._decoder.140.bias\", \"cls_decoder._decoder.141.weight\", \"cls_decoder._decoder.141.bias\", \"cls_decoder._decoder.143.weight\", \"cls_decoder._decoder.143.bias\", \"cls_decoder._decoder.144.weight\", \"cls_decoder._decoder.144.bias\", \"cls_decoder._decoder.146.weight\", \"cls_decoder._decoder.146.bias\", \"cls_decoder._decoder.147.weight\", \"cls_decoder._decoder.147.bias\", \"cls_decoder._decoder.149.weight\", \"cls_decoder._decoder.149.bias\", \"cls_decoder._decoder.150.weight\", \"cls_decoder._decoder.150.bias\", \"cls_decoder._decoder.152.weight\", \"cls_decoder._decoder.152.bias\", \"cls_decoder._decoder.153.weight\", \"cls_decoder._decoder.153.bias\", \"cls_decoder._decoder.155.weight\", \"cls_decoder._decoder.155.bias\", \"cls_decoder._decoder.156.weight\", \"cls_decoder._decoder.156.bias\", \"cls_decoder._decoder.158.weight\", \"cls_decoder._decoder.158.bias\", \"cls_decoder._decoder.159.weight\", \"cls_decoder._decoder.159.bias\", \"cls_decoder._decoder.161.weight\", \"cls_decoder._decoder.161.bias\", \"cls_decoder._decoder.162.weight\", \"cls_decoder._decoder.162.bias\", \"cls_decoder._decoder.164.weight\", \"cls_decoder._decoder.164.bias\", \"cls_decoder._decoder.165.weight\", \"cls_decoder._decoder.165.bias\", \"cls_decoder._decoder.167.weight\", \"cls_decoder._decoder.167.bias\", \"cls_decoder._decoder.168.weight\", \"cls_decoder._decoder.168.bias\", \"cls_decoder._decoder.170.weight\", \"cls_decoder._decoder.170.bias\", \"cls_decoder._decoder.171.weight\", \"cls_decoder._decoder.171.bias\", \"cls_decoder._decoder.173.weight\", \"cls_decoder._decoder.173.bias\", \"cls_decoder._decoder.174.weight\", \"cls_decoder._decoder.174.bias\", \"cls_decoder._decoder.176.weight\", \"cls_decoder._decoder.176.bias\", \"cls_decoder._decoder.177.weight\", \"cls_decoder._decoder.177.bias\", \"cls_decoder._decoder.179.weight\", \"cls_decoder._decoder.179.bias\", \"cls_decoder._decoder.180.weight\", \"cls_decoder._decoder.180.bias\", \"cls_decoder._decoder.182.weight\", \"cls_decoder._decoder.182.bias\", \"cls_decoder._decoder.183.weight\", \"cls_decoder._decoder.183.bias\", \"cls_decoder._decoder.185.weight\", \"cls_decoder._decoder.185.bias\", \"cls_decoder._decoder.186.weight\", \"cls_decoder._decoder.186.bias\", \"cls_decoder._decoder.188.weight\", \"cls_decoder._decoder.188.bias\", \"cls_decoder._decoder.189.weight\", \"cls_decoder._decoder.189.bias\", \"cls_decoder._decoder.191.weight\", \"cls_decoder._decoder.191.bias\", \"cls_decoder._decoder.192.weight\", \"cls_decoder._decoder.192.bias\", \"cls_decoder._decoder.194.weight\", \"cls_decoder._decoder.194.bias\", \"cls_decoder._decoder.195.weight\", \"cls_decoder._decoder.195.bias\", \"cls_decoder._decoder.197.weight\", \"cls_decoder._decoder.197.bias\", \"cls_decoder._decoder.198.weight\", \"cls_decoder._decoder.198.bias\", \"cls_decoder._decoder.200.weight\", \"cls_decoder._decoder.200.bias\", \"cls_decoder._decoder.201.weight\", \"cls_decoder._decoder.201.bias\", \"cls_decoder._decoder.203.weight\", \"cls_decoder._decoder.203.bias\", \"cls_decoder._decoder.204.weight\", \"cls_decoder._decoder.204.bias\", \"cls_decoder._decoder.206.weight\", \"cls_decoder._decoder.206.bias\", \"cls_decoder._decoder.207.weight\", \"cls_decoder._decoder.207.bias\", \"cls_decoder._decoder.209.weight\", \"cls_decoder._decoder.209.bias\", \"cls_decoder._decoder.210.weight\", \"cls_decoder._decoder.210.bias\", \"cls_decoder._decoder.212.weight\", \"cls_decoder._decoder.212.bias\", \"cls_decoder._decoder.213.weight\", \"cls_decoder._decoder.213.bias\", \"cls_decoder._decoder.215.weight\", \"cls_decoder._decoder.215.bias\", \"cls_decoder._decoder.216.weight\", \"cls_decoder._decoder.216.bias\", \"cls_decoder._decoder.218.weight\", \"cls_decoder._decoder.218.bias\", \"cls_decoder._decoder.219.weight\", \"cls_decoder._decoder.219.bias\", \"cls_decoder._decoder.221.weight\", \"cls_decoder._decoder.221.bias\", \"cls_decoder._decoder.222.weight\", \"cls_decoder._decoder.222.bias\", \"cls_decoder._decoder.224.weight\", \"cls_decoder._decoder.224.bias\", \"cls_decoder._decoder.225.weight\", \"cls_decoder._decoder.225.bias\", \"cls_decoder._decoder.227.weight\", \"cls_decoder._decoder.227.bias\", \"cls_decoder._decoder.228.weight\", \"cls_decoder._decoder.228.bias\", \"cls_decoder._decoder.230.weight\", \"cls_decoder._decoder.230.bias\", \"cls_decoder._decoder.231.weight\", \"cls_decoder._decoder.231.bias\", \"cls_decoder._decoder.233.weight\", \"cls_decoder._decoder.233.bias\", \"cls_decoder._decoder.234.weight\", \"cls_decoder._decoder.234.bias\", \"cls_decoder._decoder.236.weight\", \"cls_decoder._decoder.236.bias\", \"cls_decoder._decoder.237.weight\", \"cls_decoder._decoder.237.bias\", \"cls_decoder._decoder.239.weight\", \"cls_decoder._decoder.239.bias\", \"cls_decoder._decoder.240.weight\", \"cls_decoder._decoder.240.bias\", \"cls_decoder._decoder.242.weight\", \"cls_decoder._decoder.242.bias\", \"cls_decoder._decoder.243.weight\", \"cls_decoder._decoder.243.bias\", \"cls_decoder._decoder.245.weight\", \"cls_decoder._decoder.245.bias\", \"cls_decoder._decoder.246.weight\", \"cls_decoder._decoder.246.bias\", \"cls_decoder._decoder.248.weight\", \"cls_decoder._decoder.248.bias\", \"cls_decoder._decoder.249.weight\", \"cls_decoder._decoder.249.bias\", \"cls_decoder._decoder.251.weight\", \"cls_decoder._decoder.251.bias\", \"cls_decoder._decoder.252.weight\", \"cls_decoder._decoder.252.bias\", \"cls_decoder._decoder.254.weight\", \"cls_decoder._decoder.254.bias\", \"cls_decoder._decoder.255.weight\", \"cls_decoder._decoder.255.bias\", \"cls_decoder._decoder.257.weight\", \"cls_decoder._decoder.257.bias\", \"cls_decoder._decoder.258.weight\", \"cls_decoder._decoder.258.bias\", \"cls_decoder._decoder.260.weight\", \"cls_decoder._decoder.260.bias\", \"cls_decoder._decoder.261.weight\", \"cls_decoder._decoder.261.bias\", \"cls_decoder._decoder.263.weight\", \"cls_decoder._decoder.263.bias\", \"cls_decoder._decoder.264.weight\", \"cls_decoder._decoder.264.bias\", \"cls_decoder._decoder.266.weight\", \"cls_decoder._decoder.266.bias\", \"cls_decoder._decoder.267.weight\", \"cls_decoder._decoder.267.bias\", \"cls_decoder._decoder.269.weight\", \"cls_decoder._decoder.269.bias\", \"cls_decoder._decoder.270.weight\", \"cls_decoder._decoder.270.bias\", \"cls_decoder._decoder.272.weight\", \"cls_decoder._decoder.272.bias\", \"cls_decoder._decoder.273.weight\", \"cls_decoder._decoder.273.bias\", \"cls_decoder._decoder.275.weight\", \"cls_decoder._decoder.275.bias\", \"cls_decoder._decoder.276.weight\", \"cls_decoder._decoder.276.bias\", \"cls_decoder._decoder.278.weight\", \"cls_decoder._decoder.278.bias\", \"cls_decoder._decoder.279.weight\", \"cls_decoder._decoder.279.bias\", \"cls_decoder._decoder.281.weight\", \"cls_decoder._decoder.281.bias\", \"cls_decoder._decoder.282.weight\", \"cls_decoder._decoder.282.bias\", \"cls_decoder._decoder.284.weight\", \"cls_decoder._decoder.284.bias\", \"cls_decoder._decoder.285.weight\", \"cls_decoder._decoder.285.bias\", \"cls_decoder._decoder.287.weight\", \"cls_decoder._decoder.287.bias\", \"cls_decoder._decoder.288.weight\", \"cls_decoder._decoder.288.bias\", \"cls_decoder._decoder.290.weight\", \"cls_decoder._decoder.290.bias\", \"cls_decoder._decoder.291.weight\", \"cls_decoder._decoder.291.bias\", \"cls_decoder._decoder.293.weight\", \"cls_decoder._decoder.293.bias\", \"cls_decoder._decoder.294.weight\", \"cls_decoder._decoder.294.bias\", \"cls_decoder._decoder.296.weight\", \"cls_decoder._decoder.296.bias\", \"cls_decoder.out_layer.weight\", \"cls_decoder.out_layer.bias\", \"cdp_decoder._decoder.0.weight\", \"cdp_decoder._decoder.0.bias\", \"cdp_decoder._decoder.2.weight\", \"cdp_decoder._decoder.2.bias\", \"cdp_decoder._decoder.3.weight\", \"cdp_decoder._decoder.3.bias\", \"cdp_decoder._decoder.5.weight\", \"cdp_decoder._decoder.5.bias\", \"cdp_decoder._decoder.6.weight\", \"cdp_decoder._decoder.6.bias\", \"cdp_decoder._decoder.8.weight\", \"cdp_decoder._decoder.8.bias\", \"cdp_decoder._decoder.9.weight\", \"cdp_decoder._decoder.9.bias\", \"cdp_decoder._decoder.11.weight\", \"cdp_decoder._decoder.11.bias\", \"cdp_decoder._decoder.12.weight\", \"cdp_decoder._decoder.12.bias\", \"cdp_decoder._decoder.14.weight\", \"cdp_decoder._decoder.14.bias\", \"cdp_decoder._decoder.15.weight\", \"cdp_decoder._decoder.15.bias\", \"cdp_decoder._decoder.17.weight\", \"cdp_decoder._decoder.17.bias\", \"cdp_decoder._decoder.18.weight\", \"cdp_decoder._decoder.18.bias\", \"cdp_decoder._decoder.20.weight\", \"cdp_decoder._decoder.20.bias\", \"cdp_decoder._decoder.21.weight\", \"cdp_decoder._decoder.21.bias\", \"cdp_decoder._decoder.23.weight\", \"cdp_decoder._decoder.23.bias\", \"cdp_decoder._decoder.24.weight\", \"cdp_decoder._decoder.24.bias\", \"cdp_decoder._decoder.26.weight\", \"cdp_decoder._decoder.26.bias\", \"cdp_decoder._decoder.27.weight\", \"cdp_decoder._decoder.27.bias\", \"cdp_decoder._decoder.29.weight\", \"cdp_decoder._decoder.29.bias\", \"cdp_decoder._decoder.30.weight\", \"cdp_decoder._decoder.30.bias\", \"cdp_decoder._decoder.32.weight\", \"cdp_decoder._decoder.32.bias\", \"cdp_decoder._decoder.33.weight\", \"cdp_decoder._decoder.33.bias\", \"cdp_decoder._decoder.35.weight\", \"cdp_decoder._decoder.35.bias\", \"cdp_decoder._decoder.36.weight\", \"cdp_decoder._decoder.36.bias\", \"cdp_decoder._decoder.38.weight\", \"cdp_decoder._decoder.38.bias\", \"cdp_decoder._decoder.39.weight\", \"cdp_decoder._decoder.39.bias\", \"cdp_decoder._decoder.41.weight\", \"cdp_decoder._decoder.41.bias\", \"cdp_decoder._decoder.42.weight\", \"cdp_decoder._decoder.42.bias\", \"cdp_decoder._decoder.44.weight\", \"cdp_decoder._decoder.44.bias\", \"cdp_decoder.out_layer.weight\", \"cdp_decoder.out_layer.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"flag_encoder.weight\", \"mvc_decoder.gene2query.weight\", \"mvc_decoder.gene2query.bias\", \"mvc_decoder.W.weight\", \"transformer_encoder.layers.0.self_attn.Wqkv.weight\", \"transformer_encoder.layers.0.self_attn.Wqkv.bias\", \"transformer_encoder.layers.1.self_attn.Wqkv.weight\", \"transformer_encoder.layers.1.self_attn.Wqkv.bias\", \"transformer_encoder.layers.2.self_attn.Wqkv.weight\", \"transformer_encoder.layers.2.self_attn.Wqkv.bias\", \"transformer_encoder.layers.3.self_attn.Wqkv.weight\", \"transformer_encoder.layers.3.self_attn.Wqkv.bias\", \"transformer_encoder.layers.4.self_attn.Wqkv.weight\", \"transformer_encoder.layers.4.self_attn.Wqkv.bias\", \"transformer_encoder.layers.5.self_attn.Wqkv.weight\", \"transformer_encoder.layers.5.self_attn.Wqkv.bias\", \"transformer_encoder.layers.6.self_attn.Wqkv.weight\", \"transformer_encoder.layers.6.self_attn.Wqkv.bias\", \"transformer_encoder.layers.7.self_attn.Wqkv.weight\", \"transformer_encoder.layers.7.self_attn.Wqkv.bias\", \"transformer_encoder.layers.8.self_attn.Wqkv.weight\", \"transformer_encoder.layers.8.self_attn.Wqkv.bias\", \"transformer_encoder.layers.9.self_attn.Wqkv.weight\", \"transformer_encoder.layers.9.self_attn.Wqkv.bias\", \"transformer_encoder.layers.10.self_attn.Wqkv.weight\", \"transformer_encoder.layers.10.self_attn.Wqkv.bias\", \"transformer_encoder.layers.11.self_attn.Wqkv.weight\", \"transformer_encoder.layers.11.self_attn.Wqkv.bias\". \n",
      "\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([60697, 512]) from checkpoint, the shape in current model is torch.Size([60697, 128]).\n",
      "\tsize mismatch for encoder.enc_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for encoder.enc_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for value_encoder.linear1.weight: copying a param with shape torch.Size([512, 1]) from checkpoint, the shape in current model is torch.Size([128, 1]).\n",
      "\tsize mismatch for value_encoder.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for value_encoder.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for value_encoder.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for value_encoder.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for value_encoder.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.3.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.4.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.5.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.6.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.7.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.8.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.9.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.10.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.linear1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([12, 128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([12]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.linear2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for transformer_encoder.layers.11.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for decoder.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for decoder.fc.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for decoder.fc.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for decoder.fc.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for decoder.fc.4.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n",
      "scGPT - INFO - Loading param decoder.fc.4.bias with shape torch.Size([1])\n",
      "--------------------\n",
      "name: encoder.embedding.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.bias\n",
      "--------------------\n",
      "name: value_encoder.linear1.weight\n",
      "--------------------\n",
      "name: value_encoder.linear1.bias\n",
      "--------------------\n",
      "name: value_encoder.linear2.weight\n",
      "--------------------\n",
      "name: value_encoder.linear2.bias\n",
      "--------------------\n",
      "name: value_encoder.norm.weight\n",
      "--------------------\n",
      "name: value_encoder.norm.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.12.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.13.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.14.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.15.norm2.bias\n",
      "--------------------\n",
      "name: decoder.fc.0.weight\n",
      "--------------------\n",
      "name: decoder.fc.0.bias\n",
      "--------------------\n",
      "name: decoder.fc.2.weight\n",
      "--------------------\n",
      "name: decoder.fc.2.bias\n",
      "--------------------\n",
      "name: decoder.fc.4.weight\n",
      "--------------------\n",
      "name: decoder.fc.4.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.6.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.6.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.8.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.8.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.9.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.9.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.11.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.11.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.12.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.12.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.14.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.14.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.15.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.15.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.17.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.17.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.18.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.18.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.20.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.20.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.21.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.21.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.23.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.23.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.24.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.24.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.26.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.26.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.27.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.27.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.29.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.29.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.30.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.30.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.32.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.32.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.33.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.33.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.35.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.35.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.36.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.36.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.38.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.38.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.39.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.39.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.41.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.41.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.42.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.42.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.44.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.44.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.45.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.45.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.47.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.47.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.48.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.48.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.50.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.50.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.51.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.51.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.53.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.53.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.54.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.54.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.56.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.56.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.57.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.57.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.59.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.59.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.60.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.60.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.62.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.62.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.63.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.63.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.65.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.65.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.66.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.66.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.68.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.68.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.69.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.69.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.71.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.71.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.72.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.72.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.74.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.74.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.75.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.75.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.77.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.77.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.78.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.78.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.80.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.80.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.81.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.81.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.83.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.83.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.84.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.84.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.86.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.86.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.87.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.87.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.89.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.89.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.90.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.90.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.92.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.92.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.93.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.93.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.95.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.95.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.96.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.96.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.98.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.98.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.99.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.99.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.101.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.101.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.102.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.102.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.104.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.104.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.105.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.105.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.107.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.107.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.108.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.108.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.110.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.110.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.111.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.111.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.113.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.113.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.114.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.114.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.116.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.116.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.117.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.117.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.119.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.119.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.120.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.120.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.122.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.122.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.123.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.123.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.125.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.125.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.126.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.126.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.128.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.128.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.129.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.129.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.131.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.131.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.132.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.132.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.134.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.134.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.135.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.135.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.137.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.137.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.138.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.138.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.140.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.140.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.141.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.141.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.143.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.143.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.144.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.144.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.146.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.146.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.147.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.147.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.149.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.149.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.150.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.150.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.152.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.152.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.153.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.153.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.155.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.155.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.156.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.156.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.158.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.158.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.159.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.159.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.161.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.161.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.162.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.162.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.164.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.164.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.165.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.165.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.167.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.167.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.168.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.168.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.170.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.170.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.171.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.171.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.173.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.173.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.174.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.174.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.176.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.176.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.177.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.177.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.179.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.179.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.180.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.180.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.182.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.182.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.183.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.183.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.185.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.185.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.186.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.186.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.188.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.188.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.189.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.189.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.191.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.191.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.192.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.192.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.194.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.194.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.195.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.195.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.197.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.197.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.198.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.198.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.200.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.200.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.201.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.201.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.203.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.203.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.204.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.204.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.206.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.206.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.207.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.207.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.209.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.209.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.210.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.210.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.212.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.212.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.213.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.213.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.215.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.215.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.216.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.216.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.218.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.218.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.219.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.219.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.221.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.221.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.222.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.222.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.224.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.224.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.225.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.225.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.227.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.227.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.228.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.228.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.230.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.230.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.231.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.231.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.233.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.233.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.234.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.234.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.236.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.236.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.237.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.237.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.239.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.239.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.240.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.240.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.242.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.242.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.243.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.243.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.245.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.245.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.246.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.246.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.248.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.248.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.249.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.249.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.251.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.251.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.252.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.252.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.254.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.254.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.255.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.255.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.257.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.257.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.258.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.258.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.260.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.260.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.261.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.261.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.263.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.263.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.264.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.264.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.266.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.266.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.267.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.267.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.269.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.269.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.270.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.270.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.272.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.272.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.273.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.273.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.275.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.275.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.276.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.276.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.278.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.278.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.279.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.279.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.281.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.281.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.282.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.282.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.284.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.284.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.285.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.285.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.287.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.287.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.288.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.288.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.290.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.290.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.291.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.291.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.293.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.293.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.294.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.294.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.296.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.296.bias\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.weight\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.0.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.0.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.2.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.2.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.3.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.3.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.5.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.5.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.6.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.6.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.8.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.8.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.9.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.9.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.11.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.11.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.12.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.12.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.14.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.14.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.15.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.15.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.17.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.17.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.18.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.18.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.20.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.20.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.21.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.21.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.23.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.23.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.24.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.24.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.26.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.26.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.27.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.27.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.29.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.29.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.30.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.30.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.32.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.32.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.33.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.33.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.35.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.35.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.36.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.36.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.38.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.38.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.39.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.39.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.41.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.41.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.42.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.42.bias\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.44.weight\n",
      "--------------------\n",
      "name: cdp_decoder._decoder.44.bias\n",
      "--------------------\n",
      "name: cdp_decoder.out_layer.weight\n",
      "--------------------\n",
      "name: cdp_decoder.out_layer.bias\n",
      "scGPT - INFO - Total Pre freeze Params 10847811\n",
      "scGPT - INFO - Total Post freeze Params 10847811\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "# For cell density regression\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    d_model=embsize,\n",
    "    nhead=nhead,\n",
    "    d_hid=d_hid,\n",
    "    nlayers=nlayers,\n",
    "    nlayers_cls=100,\n",
    "    n_cls=1,  # Changed to 1 for regression (single continuous output)\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        # Use map_location to handle CUDA/CPU compatibility\n",
    "        model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model from {model_file}: {str(e)}\")\n",
    "        # Handle missing/unexpected keys by loading only matching parameters\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file, map_location=device)\n",
    "        \n",
    "        # Filter out the keys that don't match\n",
    "        pretrained_dict = {\n",
    "            k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        \n",
    "        # Update the model with the filtered parameters\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading param {k} with shape {v.shape}\")\n",
    "        \n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict, strict=False)  # Use strict=False to allow for missing keys\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {pre_freeze_param_count}\")\n",
    "logger.info(f\"Total Post freeze Params {post_freeze_param_count}\")\n",
    "wandb.log(\n",
    "    {\n",
    "        \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "        \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "    },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e4ea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()                                      # Reflect for cell density Might be better to change loss caracterization depending on nature of cell density ( continuous)\n",
    "# For continuous cell density prediction\n",
    "criterion_cls = nn.MSELoss()  # Mean Squared Error for regressioncriterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b734269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_mse,\n",
    "        total_cls,\n",
    "        total_cce,\n",
    "        total_mvc,\n",
    "        total_ecs,\n",
    "        total_dab,\n",
    "        total_adv_E,\n",
    "        total_adv_D,\n",
    "        total_zero_log_prob,\n",
    "        total_mvc_zero_log_prob,\n",
    "    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    total_error = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        cell_density_ = batch_data[\"cell_density\"].to(device)                                              # Adapt to cell density\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                CDP=CDP,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "            if MLM:\n",
    "                loss_mse = criterion(\n",
    "                    output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mse\n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if CLS:\n",
    "                # For regression, use MSE loss directly on the continuous output\n",
    "\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"].squeeze(), cell_density_)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                # Replace classification error with regression error metric\n",
    "                error_rate = torch.abs(output_dict[\"cls_output\"].squeeze() - cell_density_).mean().item()  # MAE\n",
    "            if CDP:\n",
    "                # For regression, use MSE loss directly on the continuous output\n",
    "                \n",
    "                loss_cls = criterion(output_dict[\"CDP_output\"].squeeze(), cell_density_)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                # Replace classification error with regression error metric\n",
    "                error_rate = torch.abs(output_dict[\"CDP_output\"].squeeze() - cell_density_).mean().item()  # MAE\n",
    "\n",
    "            if CCE:\n",
    "                loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "                loss = loss + loss_cce\n",
    "                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "            if MVC:\n",
    "                loss_mvc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "            if MVC and explicit_zero_prob:\n",
    "                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "            if ECS:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            if DAB:\n",
    "                # try weighting and separate optimizer\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                loss = loss + dab_weight * loss_dab\n",
    "                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if ADV:\n",
    "            # rerun the model for adversarial training\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            loss_adv_D = criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "            )\n",
    "            if epoch > adv_D_delay_epochs:\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_D.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # TRAINING ENCODER\n",
    "            loss_adv_E = -criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "            )\n",
    "            # NOTE: the loss is negative here because we want to maximize\n",
    "            # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "            if epoch > adv_E_delay_epochs:\n",
    "                model.zero_grad()\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_E.backward()\n",
    "                optimizer_E.step()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item() if MLM else 0.0\n",
    "        total_cls += loss_cls.item() if CLS else 0.0\n",
    "        total_cce += loss_cce.item() if CCE else 0.0\n",
    "        total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "        total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "        total_dab += loss_dab.item() if DAB else 0.0\n",
    "        total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "        total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "        total_mvc_zero_log_prob += (\n",
    "            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "        )\n",
    "        total_error += error_rate\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "            cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "            cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "            cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "            cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "            cur_zero_log_prob = (\n",
    "                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "            )\n",
    "            cur_mvc_zero_log_prob = (\n",
    "                total_mvc_zero_log_prob / log_interval\n",
    "                if MVC and explicit_zero_prob\n",
    "                else 0.0\n",
    "            )\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "                + (\n",
    "                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "                    if MVC and explicit_zero_prob\n",
    "                    else \"\"\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_cls = 0\n",
    "            total_cce = 0\n",
    "            total_mvc = 0\n",
    "            total_ecs = 0\n",
    "            total_dab = 0\n",
    "            total_adv_E = 0\n",
    "            total_adv_D = 0\n",
    "            total_zero_log_prob = 0\n",
    "            total_mvc_zero_log_prob = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Improved evaluation function with better metrics tracking and prediction handling.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            cell_density_ = batch_data[\"cell_density\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # Optionally disable during evaluation\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    CDP=CDP,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                )\n",
    "\n",
    "                output_values = output_dict[\"CDP_output\"].squeeze()\n",
    "                \n",
    "                # Regression metrics\n",
    "                loss = criterion_cls(output_values, cell_density_)\n",
    "                mae = torch.abs(output_values - cell_density_).mean()\n",
    "                mse = torch.nn.functional.mse_loss(output_values, cell_density_)\n",
    "\n",
    "                # Optional DAB loss\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                    loss += dab_weight * loss_dab\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            total_mse += mse.item()\n",
    "            total_num += 1\n",
    "\n",
    "            # Store predictions and true values for potential detailed analysis\n",
    "            predictions.append(output_values.cpu().numpy())\n",
    "            true_values.append(cell_density_.cpu().numpy())\n",
    "\n",
    "    # Compute average metrics\n",
    "    avg_loss = total_loss / total_num if total_num > 0 else 0\n",
    "    avg_mae = total_mae / total_num if total_num > 0 else 0\n",
    "    avg_mse = total_mse / total_num if total_num > 0 else 0\n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "    ss_total = ((true_values - true_values.mean()) ** 2).sum()\n",
    "    ss_residual = ((true_values - predictions) ** 2).sum()\n",
    "    r_squared = 1 - (ss_residual / ss_total) if ss_total > 0 else 0\n",
    "\n",
    "    wandb.log({\n",
    "        \"valid/loss\": avg_loss,\n",
    "        \"valid/mae\": avg_mae,\n",
    "        \"valid/mse\": avg_mse,\n",
    "        \"r_square\": r_squared,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "\n",
    "    if return_raw:\n",
    "        return predictions,true_values\n",
    "\n",
    "    return avg_loss, avg_mae, avg_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5a33011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 128, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=12, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): ReLU()\n",
       "      (14): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (16): ReLU()\n",
       "      (17): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (18): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (19): ReLU()\n",
       "      (20): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (21): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (22): ReLU()\n",
       "      (23): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (24): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (25): ReLU()\n",
       "      (26): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (27): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (28): ReLU()\n",
       "      (29): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (30): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (31): ReLU()\n",
       "      (32): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (33): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (34): ReLU()\n",
       "      (35): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (36): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (37): ReLU()\n",
       "      (38): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (39): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (40): ReLU()\n",
       "      (41): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (42): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (43): ReLU()\n",
       "      (44): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (45): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (46): ReLU()\n",
       "      (47): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (48): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (49): ReLU()\n",
       "      (50): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (51): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (52): ReLU()\n",
       "      (53): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (54): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (55): ReLU()\n",
       "      (56): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (57): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (58): ReLU()\n",
       "      (59): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (60): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (61): ReLU()\n",
       "      (62): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (63): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (64): ReLU()\n",
       "      (65): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (66): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (67): ReLU()\n",
       "      (68): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (69): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (70): ReLU()\n",
       "      (71): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (72): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (73): ReLU()\n",
       "      (74): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (75): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (76): ReLU()\n",
       "      (77): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (78): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (79): ReLU()\n",
       "      (80): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (81): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (82): ReLU()\n",
       "      (83): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (84): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (85): ReLU()\n",
       "      (86): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (87): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (88): ReLU()\n",
       "      (89): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (90): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (91): ReLU()\n",
       "      (92): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (93): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (94): ReLU()\n",
       "      (95): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (96): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (97): ReLU()\n",
       "      (98): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (99): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (100): ReLU()\n",
       "      (101): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (102): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (103): ReLU()\n",
       "      (104): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (105): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (106): ReLU()\n",
       "      (107): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (108): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (109): ReLU()\n",
       "      (110): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (111): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (112): ReLU()\n",
       "      (113): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (114): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (115): ReLU()\n",
       "      (116): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (117): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (118): ReLU()\n",
       "      (119): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (120): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (121): ReLU()\n",
       "      (122): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (123): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (124): ReLU()\n",
       "      (125): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (126): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (127): ReLU()\n",
       "      (128): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (129): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (130): ReLU()\n",
       "      (131): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (132): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (133): ReLU()\n",
       "      (134): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (135): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (136): ReLU()\n",
       "      (137): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (138): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (139): ReLU()\n",
       "      (140): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (141): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (142): ReLU()\n",
       "      (143): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (144): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (145): ReLU()\n",
       "      (146): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (147): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (148): ReLU()\n",
       "      (149): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (150): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (151): ReLU()\n",
       "      (152): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (153): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (154): ReLU()\n",
       "      (155): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (156): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (157): ReLU()\n",
       "      (158): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (159): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (160): ReLU()\n",
       "      (161): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (162): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (163): ReLU()\n",
       "      (164): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (165): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (166): ReLU()\n",
       "      (167): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (168): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (169): ReLU()\n",
       "      (170): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (171): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (172): ReLU()\n",
       "      (173): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (174): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (175): ReLU()\n",
       "      (176): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (177): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (178): ReLU()\n",
       "      (179): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (180): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (181): ReLU()\n",
       "      (182): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (183): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (184): ReLU()\n",
       "      (185): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (186): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (187): ReLU()\n",
       "      (188): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (189): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (190): ReLU()\n",
       "      (191): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (192): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (193): ReLU()\n",
       "      (194): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (195): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (196): ReLU()\n",
       "      (197): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (198): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (199): ReLU()\n",
       "      (200): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (201): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (202): ReLU()\n",
       "      (203): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (204): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (205): ReLU()\n",
       "      (206): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (207): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (208): ReLU()\n",
       "      (209): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (210): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (211): ReLU()\n",
       "      (212): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (213): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (214): ReLU()\n",
       "      (215): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (216): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (217): ReLU()\n",
       "      (218): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (219): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (220): ReLU()\n",
       "      (221): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (222): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (223): ReLU()\n",
       "      (224): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (225): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (226): ReLU()\n",
       "      (227): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (228): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (229): ReLU()\n",
       "      (230): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (231): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (232): ReLU()\n",
       "      (233): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (234): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (235): ReLU()\n",
       "      (236): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (237): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (238): ReLU()\n",
       "      (239): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (240): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (241): ReLU()\n",
       "      (242): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (243): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (244): ReLU()\n",
       "      (245): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (246): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (247): ReLU()\n",
       "      (248): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (249): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (250): ReLU()\n",
       "      (251): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (252): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (253): ReLU()\n",
       "      (254): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (255): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (256): ReLU()\n",
       "      (257): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (258): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (259): ReLU()\n",
       "      (260): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (261): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (262): ReLU()\n",
       "      (263): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (264): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (265): ReLU()\n",
       "      (266): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (267): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (268): ReLU()\n",
       "      (269): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (270): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (271): ReLU()\n",
       "      (272): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (273): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (274): ReLU()\n",
       "      (275): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (276): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (277): ReLU()\n",
       "      (278): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (279): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (280): ReLU()\n",
       "      (281): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (282): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (283): ReLU()\n",
       "      (284): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (285): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (286): ReLU()\n",
       "      (287): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (288): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (289): ReLU()\n",
       "      (290): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (291): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (292): ReLU()\n",
       "      (293): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (294): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (295): ReLU()\n",
       "      (296): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (cdp_decoder): RegressionDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): ReLU()\n",
       "      (14): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (16): ReLU()\n",
       "      (17): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (18): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (19): ReLU()\n",
       "      (20): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (21): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (22): ReLU()\n",
       "      (23): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (24): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (25): ReLU()\n",
       "      (26): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (27): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (28): ReLU()\n",
       "      (29): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (30): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (31): ReLU()\n",
       "      (32): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (33): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (34): ReLU()\n",
       "      (35): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (36): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (37): ReLU()\n",
       "      (38): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (39): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (40): ReLU()\n",
       "      (41): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (42): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (43): ReLU()\n",
       "      (44): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a503c",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   1 | 100/1140 batches | lr 0.0010 | ms/batch 134.50 | loss 79.53 | \n",
      "scGPT - INFO - | epoch   1 | 200/1140 batches | lr 0.0010 | ms/batch 130.16 | loss 37.16 | \n",
      "scGPT - INFO - | epoch   1 | 300/1140 batches | lr 0.0010 | ms/batch 129.97 | loss 36.06 | \n",
      "scGPT - INFO - | epoch   1 | 400/1140 batches | lr 0.0010 | ms/batch 129.00 | loss 36.54 | \n",
      "scGPT - INFO - | epoch   1 | 500/1140 batches | lr 0.0010 | ms/batch 129.80 | loss 37.59 | \n",
      "scGPT - INFO - | epoch   1 | 600/1140 batches | lr 0.0010 | ms/batch 126.03 | loss 35.13 | \n",
      "scGPT - INFO - | epoch   1 | 700/1140 batches | lr 0.0010 | ms/batch 126.61 | loss 37.92 | \n",
      "scGPT - INFO - | epoch   1 | 800/1140 batches | lr 0.0010 | ms/batch 128.35 | loss 38.00 | \n",
      "scGPT - INFO - | epoch   1 | 900/1140 batches | lr 0.0010 | ms/batch 126.66 | loss 37.08 | \n",
      "scGPT - INFO - | epoch   1 | 1000/1140 batches | lr 0.0010 | ms/batch 130.80 | loss 36.94 | \n",
      "scGPT - INFO - | epoch   1 | 1100/1140 batches | lr 0.0010 | ms/batch 129.97 | loss 37.57 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 199.23s | valid loss/mse 37.0440 | MAE 4.4999  | MSE 37.0440\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 37.0440\n",
      "random masking at epoch   2, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   2 | 100/1140 batches | lr 0.0009 | ms/batch 130.71 | loss 37.11 | \n",
      "scGPT - INFO - | epoch   2 | 200/1140 batches | lr 0.0009 | ms/batch 127.65 | loss 37.14 | \n",
      "scGPT - INFO - | epoch   2 | 300/1140 batches | lr 0.0009 | ms/batch 130.51 | loss 36.07 | \n",
      "scGPT - INFO - | epoch   2 | 400/1140 batches | lr 0.0009 | ms/batch 131.48 | loss 36.52 | \n",
      "scGPT - INFO - | epoch   2 | 500/1140 batches | lr 0.0009 | ms/batch 129.29 | loss 37.58 | \n",
      "scGPT - INFO - | epoch   2 | 600/1140 batches | lr 0.0009 | ms/batch 131.74 | loss 35.05 | \n",
      "scGPT - INFO - | epoch   2 | 700/1140 batches | lr 0.0009 | ms/batch 131.11 | loss 37.91 | \n",
      "scGPT - INFO - | epoch   2 | 800/1140 batches | lr 0.0009 | ms/batch 130.51 | loss 37.99 | \n",
      "scGPT - INFO - | epoch   2 | 900/1140 batches | lr 0.0009 | ms/batch 131.31 | loss 37.07 | \n",
      "scGPT - INFO - | epoch   2 | 1000/1140 batches | lr 0.0009 | ms/batch 130.84 | loss 36.94 | \n",
      "scGPT - INFO - | epoch   2 | 1100/1140 batches | lr 0.0009 | ms/batch 131.65 | loss 37.57 | \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        #intra_domain_shuffle=True,\n",
    "        #drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        #intra_domain_shuffle=False,\n",
    "        #drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_err,val = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | MAE {val_err:5.4f}  | MSE {val:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "64035ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    print(\"TEST \")\n",
    "    print(adata_spatial)\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    print(\"TEST 0\")\n",
    "    cell_id=np.array(adata.obs[\"cell_ids\"].tolist())\n",
    "    cell_density = adata.obs[\"cell_density\"].tolist()  # make sure count from 0\n",
    "    cell_density = np.array(cell_density)\n",
    "    print(\"TEST 0A\")\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    print(\"TEST 0B\")\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "    print(\"TEST 0C\")\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\"TEST 0D\")\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"cell_density\": torch.from_numpy(cell_density).long(),\n",
    "    }\n",
    "    print(\"TEST 0E\")\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        #num_workers=min(multiprocessing.cpu_count(), eval_batch_size // 2),\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(\"TEST 0F\")\n",
    "\n",
    "    model.eval()\n",
    "    predictions,true_values = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "    print(\"TEST 0G\")\n",
    "\n",
    "\n",
    "    return predictions,true_values,cell_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0b31fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST \n",
      "AnnData object with n_obs × n_vars = 161993 × 376\n",
      "    obs: 'x_centroid', 'y_centroid', 'transcript_counts', 'total_counts', 'cell_area', 'nucleus_area', 'cell_boundaries', 'nucleus_boundaries', 'batch_id', 'celltype_id', 'cell_density', 'cell_ids', 'n_counts'\n",
      "    var: 'index_column', 'gene_name', 'total_count', 'id_in_vocab'\n",
      "    layers: 'X_normed'\n",
      "TEST 0\n",
      "TEST 0A\n",
      "TEST 0B\n",
      "TEST 0C\n",
      "TEST 0D\n",
      "TEST 0E\n",
      "TEST 0F\n",
      "TEST 0G\n"
     ]
    }
   ],
   "source": [
    "preds,trues,cells_idss=test(best_model,adata_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a0ef29f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaaaficg-1', 'aaabbaka-1', 'aaabbjoo-1', ..., 'ojabeldf-1',\n",
       "       'ojacfhhg-1', 'ojacpeii-1'], dtype='<U10')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cells_idss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "edea506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1=np.hstack(preds)\n",
    "t_values_1=np.hstack(trues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f689e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cells_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaaficg-1</td>\n",
       "      <td>88.5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaabbaka-1</td>\n",
       "      <td>81.6875</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaabbjoo-1</td>\n",
       "      <td>83.3125</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaablchg-1</td>\n",
       "      <td>83.6250</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaacaicl-1</td>\n",
       "      <td>71.1875</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161988</th>\n",
       "      <td>ojaacmoj-1</td>\n",
       "      <td>93.0625</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161989</th>\n",
       "      <td>ojaaphhh-1</td>\n",
       "      <td>77.0000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161990</th>\n",
       "      <td>ojabeldf-1</td>\n",
       "      <td>78.6250</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161991</th>\n",
       "      <td>ojacfhhg-1</td>\n",
       "      <td>69.1250</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161992</th>\n",
       "      <td>ojacpeii-1</td>\n",
       "      <td>80.2500</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161993 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cells_id  prediction  true_values\n",
       "0       aaaaficg-1     88.5000           50\n",
       "1       aaabbaka-1     81.6875           65\n",
       "2       aaabbjoo-1     83.3125           70\n",
       "3       aaablchg-1     83.6250           62\n",
       "4       aaacaicl-1     71.1875           73\n",
       "...            ...         ...          ...\n",
       "161988  ojaacmoj-1     93.0625           83\n",
       "161989  ojaaphhh-1     77.0000           76\n",
       "161990  ojabeldf-1     78.6250           77\n",
       "161991  ojacfhhg-1     69.1250          111\n",
       "161992  ojacpeii-1     80.2500           81\n",
       "\n",
       "[161993 rows x 3 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'cells_id': cells_idss,\n",
    "    'prediction': preds_1,\n",
    "    'true_values': t_values_1\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5da658e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.476862627117516"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.abs(df['true_values']-np.mean(df['true_values']))\n",
    "a= np.array(a, dtype=float)\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "767def3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('predictionsss.csv', index=False)\n",
    "\n",
    "print(\"CSV file has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62ebbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
